{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgGhNLlpry0E"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aysY0swgfvTQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import convnext_base\n",
    "from collections import Counter\n",
    "from torchvision.transforms import functional as TF\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch import nn, optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.transforms import transforms, Lambda\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#!pip install git+https://github.com/facebookresearch/fvcore.git\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWsXHd55aS4d",
    "outputId": "d467adce-fcc3-459c-df48-5ff081068ac4"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygfq36eLaS4e"
   },
   "outputs": [],
   "source": [
    "dataset_address = '/home/shokoo/EchoView/Datasets/data_split/'\n",
    "files_address = '/home/shokoo/EchoView/view_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzQrpfPKQw5k",
    "outputId": "0a106ed6-a35a-4d90-824d-fae422581d30"
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join(dataset_address + '**/*/*', '*')) #numebr of all samples per patients\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data_mean = np.array([0.0847569 , 0.08113708, 0.08339239])\n",
    "avg_data_std = np.array([0.17340048, 0.17009241, 0.17395345])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRSPjiIcrx6W"
   },
   "source": [
    "### Load the csv file containing the image file names and corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz_f89aQWd1O"
   },
   "outputs": [],
   "source": [
    "info_train = pd.read_csv(os.path.join(files_address, 'csv_files/train_original.csv'))\n",
    "info_val = pd.read_csv(os.path.join(files_address, 'csv_files/val_original.csv'))\n",
    "info_test = pd.read_csv(os.path.join(files_address, 'csv_files/test_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(info_train['label']), len(info_val['label']), len(info_test['label']), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(info_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_normalized(image, label, mean, std):\n",
    "    # Un-normalize\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    image = image * std + mean  # reverse normalization\n",
    "\n",
    "    # Convert to [0, 1] and clamp to avoid display issues\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "\n",
    "    # Convert to HWC\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(f\"Label: {label.item()}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(image, label):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if image.max() <= 1.0:\n",
    "            image = image * 255.0  # Scale back to [0, 255] if needed\n",
    "        image = image.byte().permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yCgnhCLqHo5"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JR0XG56tmNN"
   },
   "source": [
    "##### Define Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZePgwSqtldr"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(nn.Module):\n",
    "    \"\"\"Add Gaussian noise to a video.\"\"\"\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "to_pil = ToPILImage()\n",
    "    \n",
    "train_transform = nn.Sequential(\n",
    "    v2.RandomHorizontalFlip(p=0.1),\n",
    "    v2.RandomVerticalFlip(p=0.2),\n",
    "    v2.RandomRotation(degrees=10),\n",
    "    transforms.RandomApply([v2.ColorJitter(brightness=0.1, contrast=0.1)], p=0.2),\n",
    "    transforms.RandomApply([torchvision.transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05))], p=0.2),\n",
    "    torchvision.transforms.RandomErasing(p=0.2),\n",
    "    transforms.RandomApply([AddGaussianNoise(0., 0.1)], p= 0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtBfvlEJwMML"
   },
   "source": [
    "### Map images to their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_info, root_dir, data_mean=0.5, data_std=0.5,\n",
    "                 use_npy=True, transform=None, remove_ecg=True, remove_static=True):\n",
    "        self.meta = csv_info\n",
    "        self.root_dir = root_dir\n",
    "        self.data_mean = data_mean\n",
    "        self.data_std = data_std\n",
    "        self.use_npy = use_npy\n",
    "        self.transform = transform\n",
    "        self.remove_ecg = remove_ecg\n",
    "        self.remove_static = remove_static\n",
    "        self.label_mapping = {\n",
    "            'PLAX': 0, 'PSAX-ves': 1, 'PSAX-base': 2, 'PSAX-mid': 3,\n",
    "            'PSAX-apical': 4, 'Apical-2ch': 5, 'Apical-3ch': 6,\n",
    "            'Apical-5ch': 7, 'Apical-4ch': 8, 'Suprasternal': 9, 'Subcostal': 10\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta.iloc[idx]\n",
    "        path = row['path']\n",
    "        case_path = path.split('/')[-1]\n",
    "        # frame_id = row['frame']\n",
    "        frame_id = 0\n",
    "        label_key = row['label']\n",
    "        label = torch.tensor([self.label_mapping[label_key]])\n",
    "        frames = []\n",
    "\n",
    "        # Load image\n",
    "        if self.use_npy:\n",
    "            npy_path = os.path.join(self.root_dir, path, \"frames.npy\")\n",
    "            frames = np.load(npy_path)\n",
    "            image = frames[frame_id]\n",
    "        else:\n",
    "            image_path = os.path.join(self.root_dir, path, f\"{case_path}_{frame_id}.png\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            # image = self.ensure_tensor_rgb(image)\n",
    "\n",
    "        # Remove ECG line\n",
    "        if self.remove_ecg:\n",
    "            image = self.remove_ecg_line(image)\n",
    "\n",
    "        # Remove static background\n",
    "        if self.remove_static:\n",
    "            image = self.remove_static_background(image, path, frames, case_path, idx)\n",
    "\n",
    "        # Tensor conversion\n",
    "        image = self.ensure_tensor_rgb(image)\n",
    "\n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Normalize\n",
    "        image = TF.normalize(image, self.data_mean, self.data_std)\n",
    "        image = TF.resize(image, size=(224,224))\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def ensure_tensor_rgb(self, image):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "            image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        elif isinstance(image, Image.Image):\n",
    "            image = TF.to_tensor(image)\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "        return image\n",
    "\n",
    "    def remove_ecg_line(self, image):\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        hsv = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(hsv, (40, 40, 40), (80, 255, 255))\n",
    "        image[mask > 0] = 0\n",
    "        return image\n",
    "\n",
    "    def remove_static_background(self, image, path, frames=None, case_path='', index=0):\n",
    "        if frames is None or len(frames) == 0:\n",
    "            ref_frames = []\n",
    "            for i in range(5):\n",
    "                img_path = os.path.join(self.root_dir, path, f\"{case_path}_{i}.png\")\n",
    "                if not os.path.exists(img_path):\n",
    "                    print('path not found')\n",
    "                    break\n",
    "                img = TF.to_tensor(Image.open(img_path).convert(\"RGB\"))\n",
    "                ref_frames.append(img)\n",
    "                \n",
    "            frames = torch.stack(ref_frames).float() / 255.0\n",
    "        else:\n",
    "            frames = torch.tensor(frames).permute(0, 3, 1, 2).float() / 255.0\n",
    "\n",
    "        if len(frames) < 2: # Not enough frames to compute mask\n",
    "            print('not enught image')\n",
    "            return image\n",
    "\n",
    "        k = min(5, frames.shape[0])\n",
    "        ref_mask = torch.zeros_like(frames[0], dtype=torch.uint8)\n",
    "        for i in range(k - 1):\n",
    "            ref_mask |= (frames[i] != frames[i + 1])\n",
    "\n",
    "        image = self.ensure_tensor_rgb(image)\n",
    "        return image * ref_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXJyCeb0ts3t"
   },
   "source": [
    "### Load the train,val,test dataset from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TTEDataset(\n",
    "    csv_info=info_train, root_dir=dataset_address, data_mean = avg_data_mean, data_std = avg_data_std, use_npy=False, remove_ecg=True, remove_static=True, \n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_data = TTEDataset(\n",
    "    csv_info=info_val, root_dir=dataset_address, data_mean = avg_data_mean, data_std = avg_data_std, use_npy=False, remove_ecg=True, remove_static=True,\n",
    ")\n",
    "\n",
    "test_data = TTEDataset( \n",
    "    csv_info=info_test, root_dir=dataset_address, data_mean = avg_data_mean, data_std = avg_data_std, use_npy=False, remove_ecg=True, remove_static=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[97]\n",
    "visualize_normalized(img, label, avg_data_mean, avg_data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nispAYROuDTh"
   },
   "source": [
    "### Create data loaders for the train and validation sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, drop_last=True, shuffle=True, pin_memory=True, num_workers= 1, persistent_workers=True)\n",
    "#, pin_memory=True, num_workers= 8, persistent_workers=True)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=32, drop_last=True, shuffle=False, pin_memory=True, num_workers= 1, persistent_workers=True)\n",
    "# collate_fn=collate_fn)\n",
    "# #pin_memory=True, num_workers= 8, persistent_workers=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, drop_last=True, shuffle=False, pin_memory=True, num_workers= 1, persistent_workers=True)\n",
    "#pin_memory=True, num_workers= 8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5J7YT_kuI_u"
   },
   "source": [
    "### Define the neural network model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_to_group = {\n",
    "    0: 'other',    # PLAX\n",
    "    1: 'psax',     # PSAX-ves\n",
    "    2: 'psax',     # PSAX-base\n",
    "    3: 'psax',     # PSAX-mid\n",
    "    4: 'psax',     # PSAX-apical\n",
    "    5: 'apical',   # Apical-2ch\n",
    "    6: 'apical',   # Apical-3ch\n",
    "    7: 'apical',   # Apical-5ch\n",
    "    8: 'apical',   # Apical-4ch\n",
    "    9: 'other',    # Suprasternal\n",
    "    10: 'other'    # Subcostal\n",
    "}\n",
    "\n",
    "group_to_labels = {\n",
    "    'apical': [5, 6, 7, 8],\n",
    "    'psax': [1, 2, 3, 4],\n",
    "    'other': [0, 9, 10]\n",
    "}\n",
    "\n",
    "# ------------ Multi-Head Classifier ------------ #\n",
    "class MultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, group_to_labels):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for group, labels in group_to_labels.items():\n",
    "            self.heads[group] = nn.Linear(feature_dim, len(labels))\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        outputs = torch.zeros(features.size(0), dtype=torch.long).to(features.device)\n",
    "        logits = torch.zeros(features.size(0), max(max(v) for v in group_to_labels.values()) + 1).to(features.device)\n",
    "\n",
    "        for group, head in self.heads.items():\n",
    "            idxs = [i for i, label in enumerate(labels) if view_to_group[label.item()] == group]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            idxs_tensor = torch.tensor(idxs, device=features.device)\n",
    "            group_features = features[idxs_tensor]\n",
    "            group_logits = head(group_features)\n",
    "            mapped_labels = group_to_labels[group]\n",
    "            for i, idx in enumerate(idxs):\n",
    "                logits[idx, mapped_labels] = group_logits[i]\n",
    "\n",
    "        return logits\n",
    "\n",
    "# ------------ Full Model ------------ #\n",
    "class MultiViewClassifier(nn.Module):\n",
    "    def __init__(self, backbone_type='convnext_small'):\n",
    "        super().__init__()\n",
    "\n",
    "        #switch between torchvision and timm models\n",
    "        if backbone_type.startswith('convnext'):\n",
    "            base = getattr(torchvision.models, backbone_type)(weights='IMAGENET1K_V1')\n",
    "            self.features = nn.Sequential(base.features, nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            feature_channels = 768 # small 768 # base 1024\n",
    "\n",
    "        else:\n",
    "            base = timm.create_model(backbone_type, pretrained=True, features_only=True) ## gets 224x224\n",
    "            self.features = nn.Sequential(base, nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            feature_channels = base.feature_info[-1]['num_chs']\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.multihead = MultiHeadClassifier(feature_dim=feature_channels, group_to_labels=group_to_labels)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = self.features(x)  # (B, 768, 1, 1)\n",
    "        x = self.flatten(x)   # (B, 768)\n",
    "        out = self.multihead(x, labels)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_type = 'convnext_small'\n",
    "model = MultiViewClassifier(backbone_type=backbone_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(2, 3, 224, 224)\n",
    "model(dummy, torch.tensor([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze initial layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model):\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_backbone(model):\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlnj51gEuSVA"
   },
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExOWtCbPuQFj",
    "outputId": "9b4ac996-270d-416f-ad57-8294e6a382ea"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# model.lstm.flatten_parameters() \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) ## changed from 0.001 to 0.0001 \n",
    "\n",
    "def update_optimizer(model, new_lr):\n",
    "    return torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=new_lr, weight_decay=1e-5)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HteOa94uZhZ"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2VwektM9oa4"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, mode='validation'):\n",
    "  model.eval()\n",
    "  total_correct = 0\n",
    "  total_loss = 0\n",
    "  # BATCH_SIZE = 64\n",
    "  total = 0\n",
    "  for i, (images, labels) in enumerate(loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    labels = labels.squeeze(1)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(images, labels)\n",
    "      loss = criterion(outputs, labels)\n",
    "      total += images.size(0)\n",
    "      _, predictions = outputs.max(1)\n",
    "      total_correct += (labels == predictions).sum()\n",
    "      total_loss += loss.item() * images.size(0)\n",
    "      # print('Valid \\t',  predictions, labels, labels == predictions, total_correct, total, images.size(0), '\\n')\n",
    "\n",
    "  # print(total_correct, total, accuracy)\n",
    "  loss = total_loss / total\n",
    "  accuracy = total_correct / total\n",
    "  print(f'{mode} epoch {epoch}: Loss({loss:6.4f}),  Accuracy ({accuracy:6.4f}))')\n",
    "  return accuracy, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "OGRUWRhJuXoU",
    "outputId": "c6b87a67-8a5f-47fa-8236-e9caecd7b846"
   },
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "backbone_freeze_epoch = 5\n",
    "accs = []\n",
    "losses = []\n",
    "val_loss = 0.0\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# freeze_backbone(model)\n",
    "# print(\"Backbone frozen.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  total = 0\n",
    "  running_loss = 0.\n",
    "  running_corrects = 0\n",
    "\n",
    "  # if epoch == backbone_freeze_epoch:\n",
    "  #   unfreeze_backbone(model)\n",
    "  #   update_optimizer(model, new_lr=1e-5)\n",
    "  #   print(\"Backbone unfrozen, optimizer updated.\")\n",
    "\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images, labels)\n",
    "    labels = labels.squeeze(1)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total += images.size(0)\n",
    "    _, predictions = outputs.max(1)\n",
    "    running_loss += loss.item() * images.size(0)\n",
    "    running_corrects += (predictions == labels).sum()\n",
    "\n",
    "  epoch_loss = running_loss / total\n",
    "  epoch_acc = running_corrects / total\n",
    "  accs.append(epoch_acc)\n",
    "  losses.append(epoch_loss)\n",
    "  # loss = total_loss / total\n",
    "  print(f'Train epoch {epoch}: Loss({epoch_loss:6.4f}, Accuracy ({epoch_acc:6.5f}) )')\n",
    "  val_acc, val_loss = evaluate(model, val_loader, device, mode='valid')\n",
    "  accs.append(val_acc)\n",
    "  losses.append(val_loss)\n",
    "\n",
    "  print('---')\n",
    "\n",
    "  if(val_loss < best_val_loss):\n",
    "    best_acc = val_acc\n",
    "    best_val_loss = val_loss\n",
    "    epochs_no_improve = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "#     torch.save(model.state_dict(), (f'/home/student/shokoofa/Downloads/res_model{epoch}.pth'))\n",
    "  else:\n",
    "    epochs_no_improve += 1\n",
    "\n",
    "  if epochs_no_improve > 2 and epoch > 7:\n",
    "    print('no further improvement ', best_acc)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model if best_model != None else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_name': 'f{model.__class__.__name__}',\n",
    "    'num_classes': 11,\n",
    "    'discription': f'trained with {backbone_type} backbone, including {[i.__class__.__name__ for i in train_transform]} in augmentation, images shape is {img.shape}',\n",
    "    'accuracyOnVal' : round(val_acc.item(),3),\n",
    "    'date': str(datetime.datetime.now()).split('.')[0],\n",
    "    'state_dict': model.state_dict(),\n",
    "    'model' : model,\n",
    "}, os.path.join(files_address, 'notebooks/saved_models', f'model_{model.__class__.__name__}-bb_({backbone_type})-acc_{round(best_acc.item(),3)}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_info = torch.load(os.path.join(files_address, 'notebooks/saved_models', f'model_{model.__class__.__name__}-bb_({backbone_type})-acc_{round(val_acc.item(),3)}.pth'))\n",
    "model = loaded_info['model']\n",
    "description = loaded_info['discription']\n",
    "description\n",
    "\n",
    "# state_dict = torch.load('/home/shokoo/EchoView/video_class/singleframe/test3.pth', map_location='cpu')\n",
    "# new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "# model.load_state_dict(new_state_dict)\n",
    "# model.to(device)\n",
    "# val_acc, val_loss = evaluate(model, val_loader, device, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, mode='Test'):\n",
    "  model.eval()\n",
    "  total_correct = 0\n",
    "  total_loss = 0\n",
    "  # BATCH_SIZE = 64\n",
    "  total = 0\n",
    "  for i, (images, labels) in enumerate(loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    labels = labels.squeeze(1)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(images, labels)\n",
    "      loss = criterion(outputs, labels)\n",
    "      total_loss += loss.item() * images.size(0)\n",
    "      total += images.size(0) \n",
    "      _, predictions = outputs.max(1)\n",
    "      total_correct += (labels == predictions).sum()\n",
    "\n",
    "  # print(total_correct, total, accuracy)\n",
    "  loss = total_loss / total\n",
    "  accuracy = total_correct / total\n",
    "  print(f'{mode} Loss({loss:6.4f}),  Accuracy ({accuracy:6.4f}))')\n",
    "  return accuracy, loss\n",
    "\n",
    "evaluate(model, loader= test_loader, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = [], []\n",
    "\n",
    "valid_labels = {'PLAX': 0, 'PSAX-ves' : 1, 'PSAX-base' : 2, 'PSAX-mid' : 3,\n",
    "                              'PSAX-apical' : 4, 'Apical-2ch' : 5, 'Apical-3ch' : 6,\n",
    "                              'Apical-5ch' : 7, 'Apical-4ch' : 8, 'Suprasternal' : 9, 'Subcostal' : 10\n",
    "               }\n",
    "\n",
    "labels_name = valid_labels.keys()\n",
    "model.eval()\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    labels = labels.squeeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, labels)\n",
    "        _, predictions = outputs.max(1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "        \n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_name, yticklabels=labels_name )\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', xticklabels=labels_name, yticklabels=labels_name )\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
